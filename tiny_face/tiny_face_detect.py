# -*- coding: utf-8 -*-
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
import tiny_face.tiny_face_model as tiny_face_model
import tiny_face.util as util
from argparse import ArgumentParser
import cv2
import scipy.io
import numpy as np
import matplotlib.pyplot as plt
import cv2
import pickle

import pylab as pl
import time
import os
import sys
from scipy.special import expit
import glob

MAX_INPUT_DIM = 5000.0

class TinyFace:

    def __init__(self, prob_thresh=0.5, nms_thresh=0.1):
        """Detect faces in images.
        Args:
            prob_thresh:
                The threshold of detection confidence.
            nms_thresh:
                    The overlap threshold of non maximum suppression
            weight_file_path:
                    A pretrained weight file in the pickle format
                    generated by matconvnet_hr101_to_tf.py.
            data_dir:
                    A directory which contains images.
            output_dir:
                    A directory into which images with detected faces are output.
            lw:
                    Line width of bounding boxes. If zero specified,
                    this is determined based on confidence of each detection.
            display:
                    Display tiny face images on window.
        Returns:
            None.
        """

        # with tf.Graph().as_default():

        weight_file_path = 'tiny_face/hr_res101.pkl'
        self.prob_thresh = prob_thresh
        self.nms_thresh = nms_thresh

        # placeholder of input images. Currently batch size of one is supported.
        self.x = tf.placeholder(tf.float32, [1, None, None, 3]) # n, h, w, c

        # Create the tiny face model which weights are loaded from a pretrained model.
        model = tiny_face_model.Model(weight_file_path)
        self.score_final = model.tiny_face(self.x)

        # Load an average image and clusters(reference boxes of templates).
        with open(weight_file_path, "rb") as f:
            _, mat_params_dict = pickle.load(f)

        self.average_image = model.get_data_by_key("average_image")
        self.clusters = model.get_data_by_key("clusters")
        self.clusters_h = self.clusters[:, 3] - self.clusters[:, 1] + 1
        self.clusters_w = self.clusters[:, 2] - self.clusters[:, 0] + 1
        self.normal_idx = np.where(self.clusters[:, 4] == 1)

        self.sess = tf.Session()
        self.sess.run(tf.global_variables_initializer())

    def calc_scales(self, raw_img):
        '''

        Calculates the different scales of a frame for the CNN to find faces inself.

        '''

        raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]
        min_scale = min(np.floor(np.log2(np.max(self.clusters_w[self.normal_idx] / raw_w))),
                        np.floor(np.log2(np.max(self.clusters_h[self.normal_idx] / raw_h))))
        max_scale = min(1.0, -np.log2(max(raw_h, raw_w) / MAX_INPUT_DIM))
        scales_down = pl.frange(min_scale, 0, 1.)
        scales_up = pl.frange(0.5, max_scale, 0.5)
        scales_pow = np.hstack((scales_down, scales_up))
        scales = np.power(2.0, scales_pow)
        return scales

    def get_face_box(self, raw_img, face_ROI):

        # start = time.time()

        frame_height = raw_img.shape[0]
        frame_width = raw_img.shape[1]

        # extract face bounding box coords
        x1 = face_ROI[0]
        y1 = face_ROI[1]
        x2 = face_ROI[2]
        y2 = face_ROI[3]

        # crop the image
        raw_face_roi = raw_img[y1:y2,x1:x2]

        face_roi = cv2.cvtColor(raw_face_roi, cv2.COLOR_BGR2RGB)

        face_roi_f = face_roi.astype(np.float32)

        # initialize output
        bboxes = np.empty(shape=(0, 5))

        # need to run frame at different scales through cnn to find all faces
        scales = self.calc_scales(raw_img)

        # don't need to do all scales, do every other and not the last one
        for s in scales[1:-2:2]:
            # print("Processing {} at scale {:.4f}".format(filename, s))

            img = cv2.resize(face_roi_f, (0, 0), fx=s, fy=s, interpolation=cv2.INTER_LINEAR)
            img = img - self.average_image
            img = img[np.newaxis, :]

            # we don't run every template on every scale ids of templates to ignore
            tids = list(range(4, 12)) + ([] if s <= 1.0 else list(range(18, 25)))
            ignoredTids = list(set(range(0, self.clusters.shape[0])) - set(tids))

            # run through the net
            score_final_tf = self.sess.run(self.score_final, feed_dict={self.x: img})

            # collect scores
            score_cls_tf, score_reg_tf = score_final_tf[:, :, :, :25], score_final_tf[:, :, :, 25:125]
            prob_cls_tf = expit(score_cls_tf)
            prob_cls_tf[0, :, :, ignoredTids] = 0.0

            def _calc_bounding_boxes():
                # threshold for detection
                _, fy, fx, fc = np.where(prob_cls_tf > self.prob_thresh)

                # interpret heatmap into bounding boxes
                cy = fy * 8 - 1
                cx = fx * 8 - 1
                ch = self.clusters[fc, 3] - self.clusters[fc, 1] + 1
                cw = self.clusters[fc, 2] - self.clusters[fc, 0] + 1

                # extract bounding box refinement
                Nt = self.clusters.shape[0]
                tx = score_reg_tf[0, :, :, 0:Nt]
                ty = score_reg_tf[0, :, :, Nt:2*Nt]
                tw = score_reg_tf[0, :, :, 2*Nt:3*Nt]
                th = score_reg_tf[0, :, :, 3*Nt:4*Nt]

                # refine bounding boxes
                dcx = cw * tx[fy, fx, fc]
                dcy = ch * ty[fy, fx, fc]
                rcx = cx + dcx
                rcy = cy + dcy
                rcw = cw * np.exp(tw[fy, fx, fc])
                rch = ch * np.exp(th[fy, fx, fc])

                scores = score_cls_tf[0, fy, fx, fc]
                tmp_bboxes = np.vstack((rcx - rcw / 2, rcy - rch / 2, rcx + rcw / 2, rcy + rch / 2))
                tmp_bboxes = np.vstack((tmp_bboxes / s, scores))
                tmp_bboxes = tmp_bboxes.transpose()
                return tmp_bboxes

            tmp_bboxes = _calc_bounding_boxes()
            bboxes = np.vstack((bboxes, tmp_bboxes)) # <class 'tuple'>: (5265, 5)

        # non maximum suppression
        # refind_idx = util.nms(bboxes, nms_thresh)

        refind_idx = tf.image.non_max_suppression(tf.convert_to_tensor(bboxes[:, :4], dtype=tf.float32),
                                               tf.convert_to_tensor(bboxes[:, 4], dtype=tf.float32),
                                               max_output_size=bboxes.shape[0], iou_threshold=self.nms_thresh)

        refind_idx = self.sess.run(refind_idx)
        refined_bboxes = bboxes[refind_idx]

        # the refined bboxes are for the cropped image, need to correct for offset for orig. frame
        face_box = None
        face_bounding_boxes = self.correct_offset(face_ROI, refined_bboxes)

        # if face found, then enlarge it
        if face_bounding_boxes:
            # only use the first face found from the face ROI, since we cropped image anyways
            face_box = face_bounding_boxes[0]
            x1, y1, x2, y2 = self.enlarge_box(face_bounding_boxes[0])

            # need to make sure all coordinates are in bound first, and are ints
            x1 = max(0, x1)
            y1 = max(0, y1)
            x2 = min(x2, frame_width)
            y2 = min(y2, frame_height)
            face_box = (x1,y1,x2,y2)

        return face_box

    def enlarge_box(self, raw_face_box):
        '''
        Increase the size of the face box, but still ensure inbounds.

        '''

        # paramters tuned by subsampling test data
        width_multiplier = 1.4
        height_multiplier = 1.2

        # unpack face box coords
        x1,y1,x2,y2 = raw_face_box

        curr_height = y2-y1
        curr_width = x2-x1

        center_x = x1 + curr_width/2
        center_y = y1 + curr_height/2

        new_height = curr_height * height_multiplier
        new_width = curr_width * width_multiplier

        x1 = center_x - new_width/2
        x2 = center_x + new_width/2

        y1 = center_y - new_height/2
        y2 = center_y + new_height/2

        return (int(x1), int(y1), int(x2), int(y2))

    def correct_offset(self, face_ROI, refined_bboxes):
        '''
        Corrects the offset from the tiny face detectory CNN by the original face_ROI starting position

        '''

        corrected_face_boxes = []

        roi_startX = face_ROI[0]
        roi_startY = face_ROI[1]
        roi_endX = face_ROI[2]
        roi_endY = face_ROI[3]

        for face in refined_bboxes:

            # extract each coordinate and add the offset to the original frame
            x1 = int(face[0] + roi_startX)
            y1 = int(face[1] + roi_startY)
            x2 = int(face[2] + roi_startX)
            y2 = int(face[3] + roi_startY)

            corrected_face_boxes.append((x1,y1,x2,y2))

        return corrected_face_boxes
